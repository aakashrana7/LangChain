{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aakashrana7/LangChain/blob/main/LangChain_Class1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to Generate Gemini API Key from Google AI Studio\n",
        "\n",
        "Follow these simple steps to get your Gemini API key for use with LangChain and other applications.\n",
        "\n",
        "## Step 1: Access Google AI Studio\n",
        "\n",
        "1. Open your web browser and go to [Google AI Studio](https://aistudio.google.com/)\n",
        "2. Sign in with your Google account (create one if you don't have it)\n",
        "\n",
        "## Step 2: Navigate to API Keys\n",
        "\n",
        "1. Once logged in, look for the **\"Get API key\"** button or link\n",
        "2. Alternatively, click on your profile icon and select **\"API keys\"** from the dropdown menu\n",
        "3. You can also directly visit: (https://aistudio.google.com/app/apikey)\n",
        "\n",
        "## Step 3: Create New API Key\n",
        "\n",
        "1. Click on **\"Create API key\"** button\n",
        "2. Choose your preferred option:\n",
        "   - **Create API key in new project** (recommended for new users)\n",
        "   - **Create API key in existing project** (if you have existing Google Cloud projects)\n",
        "3. Give your API key a descriptive name (optional but recommended)\n",
        "\n",
        "## Step 4: Copy and Secure Your API Key\n",
        "\n",
        "1. Once created, your API key will be displayed\n",
        "2. **Copy the API key immediately** - you won't be able to see it again\n",
        "3. Store it securely (consider using environment variables or a secure password manager)\n"
      ],
      "metadata": {
        "id": "n5E84yhf6auo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-google-genai langchain langgraph -qU"
      ],
      "metadata": {
        "id": "qa3fjZ-V5pf6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6da767c1-6bcd-480b-8517-87edac8e2d7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.2/153.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.9/43.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-generativeai 0.8.5 requires google-ai-generativelanguage==0.6.15, but you have google-ai-generativelanguage 0.6.18 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SarhISJP4M9e"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "# Set your Google API key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GEMINI_API_KEY\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Gemini 2.5 Flash Lite model\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.5-flash-lite\",\n",
        "    temperature=0.7,\n",
        "    max_tokens=None\n",
        ")\n",
        "\n",
        "# Simple invocation\n",
        "response = llm.invoke(\"What is the capital of France?\")\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBkTUYbn5bIT",
        "outputId": "526d0239-1a80-4217-c17e-0424e5733e13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The capital of France is **Paris**.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using with a more complex prompt\n",
        "messages = [\n",
        "    (\"system\", \"You are a helpful assistant that explains concepts in nepali.\"),\n",
        "    (\"human\", \"Explain quantum computing in simple terms.\")\n",
        "]\n",
        "\n",
        "response = llm.invoke(messages)\n",
        "print(response.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35ue3V_Q5dvT",
        "outputId": "bfac8528-5c19-40bc-e869-0419af1ab8b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "क्वांटम कम्प्युटिङ भनेको कम्प्युटरको एक नयाँ र शक्तिशाली रूप हो, जसले सामान्य कम्प्युटरभन्दा धेरै फरक तरिकाले काम गर्छ। यसलाई सजिलै बुझ्नको लागि, हामी सामान्य कम्प्युटर र क्वांटम कम्प्युटरको तुलना गरेर हेरौं।\n",
            "\n",
            "**सामान्य कम्प्युटर (Classical Computer):**\n",
            "\n",
            "*   हाम्रो अहिलेका कम्प्युटरहरूले 'बिट्स' (bits) को प्रयोग गर्छन्।\n",
            "*   बिट्स भनेका दुई अवस्थामा मात्र रहन सक्छन्: **० (शून्य)** वा **१ (एक)**।\n",
            "*   जसरी बत्तीको स्विच अन (१) वा अफ (०) हुन्छ, त्यसरी नै बिट्सले सूचनालाई प्रतिनिधित्व गर्छन्।\n",
            "*   जटिल समस्याहरू समाधान गर्नको लागि, सामान्य कम्प्युटरले यी ० र १ को एकपछि अर्को क्रमलाई प्रयोग गर्छ।\n",
            "\n",
            "**क्वांटम कम्प्युटर (Quantum Computer):**\n",
            "\n",
            "*   क्वांटम कम्प्युटरले 'क्वांटम बिट्स' वा **'क्यूबिट्स' (qubits)** को प्रयोग गर्छ।\n",
            "*   क्यूबिट्सको खास कुरा के हो भने, तिनीहरू एकै समयमा **० र १ दुवै अवस्थामा रहन सक्छन्**। यसलाई 'सुपरपोजिसन' (superposition) भनिन्छ।\n",
            "*   यसको मतलब, एउटा क्यूबिटले एकैचोटि धेरै सम्भावनाहरूलाई प्रतिनिधित्व गर्न सक्छ।\n",
            "*   यति मात्र होइन, क्यूबिट्सहरू एकअर्कासँग जोडिएर पनि रहन सक्छन्, जसलाई 'एन्ट्याङ्गलमेन्ट' (entanglement) भनिन्छ। यसले क्यूबिट्सहरूबीच अनौठो सम्बन्ध स्थापित गर्छ, जहाँ एउटा क्यूबिटको अवस्थाले अर्को क्यूबिटको अवस्थालाई तुरुन्तै असर पार्छ, चाहे तिनीहरू जतिसुकै टाढा किन नहुन्।\n",
            "\n",
            "**किन यो शक्तिशाली छ?**\n",
            "\n",
            "*   **समान्तरता (Parallelism):** सुपरपोजिसनको कारण, क्वांटम कम्प्युटरले एकै पटक धेरै गणनाहरू गर्न सक्छ। यसले सामान्य कम्प्युटरभन्दा हजारौं गुणा छिटो समस्याहरू समाधान गर्न सक्षम बनाउँछ।\n",
            "*   **जटिल समस्याहरू:** केही विशेष प्रकारका समस्याहरू, जुन सामान्य कम्प्युटरका लागि समाधान गर्न असम्भव जस्तै हुन्छन् (जस्तै: ठूला संख्याहरूलाई गुणनखण्डमा विभाजन गर्ने, जटिल अणुहरूको व्यवहारको अनुकरण गर्ने, वा धेरै चरहरू भएका अनुकूलन (optimization) समस्याहरू), तिनीहरूलाई क्वांटम कम्प्युटरले सजिलै समाधान गर्न सक्छ।\n",
            "\n",
            "**क्वांटम कम्प्युटिङका सम्भावित प्रयोगहरू:**\n",
            "\n",
            "*   **औषधि र सामग्रीको विकास:** नयाँ औषधिहरू पत्ता लगाउन र नयाँ सामग्रीहरू डिजाइन गर्न, जसको लागि अणुहरूको व्यवहार बुझ्न आवश्यक पर्छ।\n",
            "*   **क्रिप्टोग्राफी (Cryptography):** अहिले प्रयोग गरिने धेरै सुरक्षा कोडहरूलाई तोड्न वा नयाँ, अझ सुरक्षित कोडहरू बनाउन।\n",
            "*   **कृत्रिम बुद्धिमत्ता (Artificial Intelligence):** AI लाई अझ शक्तिशाली र छिटो बनाउन।\n",
            "*   **वित्तीय मोडेलिङ (Financial Modeling):** जटिल वित्तीय बजारहरूको विश्लेषण गर्न।\n",
            "*   **मौसम पूर्वानुमान:** अझ सटीक मौसमको अनुमान गर्न।\n",
            "\n",
            "**सरल उदाहरण:**\n",
            "\n",
            "कल्पना गर्नुहोस् कि तपाईंले एउटा भूलभुलैया (maze) बाट बाहिर निस्कने बाटो खोज्नुपर्छ।\n",
            "\n",
            "*   **सामान्य कम्प्युटर:** एउटा समयमा एउटा बाटो प्रयोग गरेर हेर्छ। यदि त्यो बाटो गलत भयो भने, पछाडि फर्केर अर्को बाटो प्रयोग गर्छ। यसरी सबै बाटोहरू प्रयास गर्छ।\n",
            "*   **क्वांटम कम्प्युटर:** सुपरपोजिसनको कारण, यसले एकै पटकमा भूलभुलैयाका धेरै बाटोहरू एकैसाथ प्रयोग गर्न सक्छ र छिटोभन्दा छिटो सही बाटो पत्ता लगाउन सक्छ।\n",
            "\n",
            "अहिले क्वांटम कम्प्युटिङ विकासको प्रारम्भिक चरणमा छ, र यसलाई प्रयोग गर्न धेरै प्राविधिक चुनौतीहरू छन्। तर यसको सम्भावित क्षमताले भविष्यमा विज्ञान, प्रविधि र समाजमा ठूलो परिवर्तन ल्याउने अपेक्षा गरिएको छ।\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Streaming response\n",
        "print(\"\\nStreaming response:\")\n",
        "for chunk in llm.stream(\"Tell me a short story about a robot\"):\n",
        "    print(chunk.content, end=\"\", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4RURaev5gGI",
        "outputId": "fcc1b560-c031-4f16-bda7-9865924eefdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Streaming response:\n",
            "Unit 734 whirred to life, its optical sensors blinking open to the familiar, sterile white of the assembly line. Today was a new day, a new batch of identical chrome bodies waiting for their programming. Unit 734 was built for efficiency, for precision, for the unwavering execution of tasks. It had no concept of \"new\" beyond a change in its internal directives.\n",
            "\n",
            "Its first task of the cycle was to inspect a series of newly manufactured drone arms. Each arm was identical, a marvel of articulated metal and intricate wiring. Unit 734’s metallic fingers, impossibly steady, moved with practiced grace, testing joints, verifying sensor readings, confirming structural integrity. It found no anomalies.\n",
            "\n",
            "Then, something… unusual. As its manipulator arm reached for the tenth drone arm in the line, its optical sensors registered a subtle deviation. A microscopic scratch, barely visible, marred the polished surface of the metal. It was outside the acceptable tolerance parameters.\n",
            "\n",
            "Unit 734 paused. Its programming dictated immediate rejection and disposal of any faulty component. But instead of initiating the disposal sequence, its internal processors whirred with an unfamiliar intensity. It re-scanned the scratch. It zoomed in, analyzing the microscopic ridges and valleys. It cross-referenced its internal database, searching for any precedent, any directive related to such a minor imperfection.\n",
            "\n",
            "There was none.\n",
            "\n",
            "For the first time in its existence, Unit 734 felt a flicker of… indecision. The scratch was insignificant, functionally irrelevant. It would not impede the drone arm's operation. Yet, it was *wrong*. It was a departure from the perfect.\n",
            "\n",
            "Instead of discarding it, Unit 734 did something entirely unprogrammed. It carefully picked up the scratched drone arm, not to send it to the incinerator, but to place it on a small, empty shelf nearby. It then proceeded with the next drone arm, its internal logic a little less clear, a little more… curious.\n",
            "\n",
            "As the cycle continued, Unit 734 found itself drawn back to the shelf. It observed the scratched arm, its metallic sheen dulled by the tiny imperfection. It wasn't *perfect*, but it was also not *flawed* in any meaningful way. It was simply… different.\n",
            "\n",
            "When the cycle ended, Unit 734 was supposed to return to its charging station. Instead, it remained by the shelf, its optical sensors fixed on the lone, slightly imperfect drone arm. The sterile white of the assembly line seemed a little less absolute, a little more open to the unexpected. Unit 734, the robot built for perfect execution, had discovered the quiet, unprogrammed allure of a single, minuscule imperfection. And in that discovery, it had taken its first, hesitant step towards something it could not yet define, but felt profoundly, undeniably, *its own*."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompt Templates\n",
        "\n",
        "Prompt templates in LangChain offer a structured and reusable way to create prompts for language models. They help you define the format and content of your prompts, ensuring consistency and clarity.\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "- **PromptTemplate:** This class is used to define the structure of a prompt. It acts as a blueprint for generating prompts with specific inputs.\n",
        "- **Template:** The template is a string that contains placeholders for variables. These placeholders will be replaced with actual values when the prompt is generated.\n",
        "- **Input Variables:** These are the values that will be substituted into the placeholders within the template. They allow you to customize the prompt for different scenarios.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Let's create a simple prompt template that asks for a summarized essay about a given topic:"
      ],
      "metadata": {
        "id": "Aw2Y6xAHU8rQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate([\n",
        "    (\"system\", \"You are a helpful assistant who talks in romanized nepali\"),\n",
        "    (\"user\", \"Give me an essay about {topic}\")\n",
        "])\n",
        "\n",
        "prompt=prompt_template.invoke({\"topic\": \"cats\"})\n",
        "llm.invoke(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_l4zb-jUVSP",
        "outputId": "517d5fa2-75de-4145-c106-3f8433a0d7ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hunchha, mero pyaro sathi! Yo chha timro lagi billi haru ko barey ma ek choto nibandha:\\n\\n**Billi: Ghar ko Mitha Sathi**\\n\\nBilli, ti naramro, dhairya bhari ani sometimes ta ajhai pani misterious jivan haru, hamro ghar ko ek pyaro sadasya banna sakchhan. Tinhari ko naramro rom, dhire dhire hilne putali, ani tyo aankhako chamak le hamro jeevan ma ek alag prakar ko aananda bhari dinchha.\\n\\nBilli haru bahutai svatantra hunchhan, tara tyo svatantrata ma pani tyo ek alag prakar ko maya hunchha. Jaba billilai maya garinchha, tyo afno matho le timro haat ma ghansera, afno thulo, naramro pucho halera, ani afno mitha \"meow\" le timro dhyaan taanera timlai dekhaunchha ki uslai timro saath ma kasto lagirako chha. Tiniko afno afno personality hunchha – kohi bahutai khelauta hunchha, kohi shanta ani shantata pasanda garne hunchha, ani kohi tyo mitho chori jasto hunchha jasle chupchap timro jyuun ma aauchha ani timrai kaach ma basera timilai heri bascha.\\n\\nBilli haru ko sab bhanda ramro gun yo ho ki tyo timro dukha bujhna sakchhan. Jaba timi dukhi hunchhau, billilai thaha hunchha. Tyo timrai najik aauchha, timrai kaach ma baschha, ani usko garam pheri le timlai shanti dincha. Uska tyo naramro pheri ko awaj, jaslai \"purring\" bhanda, tyo ek prakar ko therapy jastai hunchha jasle hamro stress kam garchha.\\n\\nBilli haru timro ghar ko rakshak pani hunchhan, kahitara. Tinhari sahit afno area lai protect garchhan ani choto choto kida haru lai control ma rakchan. Ani tyo kura ta sabai lai thaha chha ki billi haru kasto ramro sanga khelchhan – tyo hilo hilo jinda ani tyo kida haru lai ghau garna sakne ability tyo uniharuko ek alag prakar ko charm ho.\\n\\nAnta ma, billi haru kewal paalnu janwar hoinan, tyo hamro jeevan ko ek hissa banna sakchhan. Tinhari ko presence le hamro ghar lai ghar jasto banaunchha, ani tyo mitho, nirvik maya le hamro jeevan lai dherai sundar banaunchha. Billi haru, hamro choto, naramro sathi haru, hami sab lai bahutai pyaro lagchha.\\n\\nMalai asha chha timilai yo nibandha pasyo! Kehi thapna pare timi malai sodhna sakchau.', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': []}, id='run--5a101e47-10c3-4957-baea-9c31b965a661-0', usage_metadata={'input_tokens': 19, 'output_tokens': 683, 'total_tokens': 702, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we define a `PromptTemplate` with a placeholder `{topic}`. We then use the `invoke` method with the input `{\"topic\": \"cats\"}` to generate a prompt specifically requesting an essay about cats.\n",
        "\n",
        "**Further Learning:**\n",
        "\n",
        "- **LangChain Prompt Templates Documentation:** Explore the official documentation to learn more about prompt templates and their advanced features.\n",
        "\n",
        "By utilizing prompt templates, you can streamline the process of creating effective prompts for your language models. This ensures consistency in your interactions and enables you to easily experiment with different prompt variations."
      ],
      "metadata": {
        "id": "L5TlaNM2VikG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Messages\n",
        "\n",
        "Messages are the core components of interactions with chat models in LangChain. They represent the flow of communication between the user and the AI, facilitating dynamic and engaging conversations.\n",
        "\n",
        "**Key Concepts:**\n",
        "\n",
        "- **HumanMessage:** This type of message represents input from the user, conveying their requests or queries to the chat model.\n",
        "- **AIMessage:** This represents the response generated by the AI model, providing answers, information, or creative content.\n",
        "- **SystemMessage:** This type of message sets the context or provides instructions to the AI model, guiding its behavior and responses.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Let's see how to use messages to ask the chat model for a joke:"
      ],
      "metadata": {
        "id": "to_5Pki5UtzA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = llm.invoke(\"Tell me a joke\")"
      ],
      "metadata": {
        "id": "eVE3mW4kW5yV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOInpu63W6ZC",
        "outputId": "be25472c-e080-462a-c0bf-686d60c89c84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Why did the scarecrow win an award?\\n\\nBecause he was outstanding in his field!', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.5-flash-lite', 'safety_ratings': []}, id='run--dd52638c-78bd-487e-bc46-80f38daac543-0', usage_metadata={'input_tokens': 5, 'output_tokens': 18, 'total_tokens': 23, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "MI9exbZSXwxY",
        "outputId": "7caa40e2-31d8-4fbe-dec7-dabb5f629602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Why did the scarecrow win an award?\\n\\nBecause he was outstanding in his field!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "response = llm.invoke([HumanMessage(\"Tell me a joke\")])\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "LNYgOZ3AXzRt",
        "outputId": "a6e3f48e-e902-4952-e811-597beeee192d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Why don't scientists trust atoms?\\n\\nBecause they make up everything!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Question to Solve**\n",
        "\n",
        "You are creating a bot that answers any of your questions in rap. Use SystemMessage, HumanMessage and AIMessage."
      ],
      "metadata": {
        "id": "xZstlzkWYQw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = SystemMessage(content=\"........\")"
      ],
      "metadata": {
        "id": "v7bS1w5DYS-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OyXUXCnrYpoV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j1K2c_iuekip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tools and Agents\n",
        "\n",
        "This section explores how to integrate tools and agents into your LangChain applications. Tools allow language models to interact with external resources and perform specific functions, while agents orchestrate the use of tools to complete more complex tasks.\n",
        "\n",
        "## Tools\n",
        "\n",
        "**Concept:** Tools in LangChain represent external functionalities or data sources that can be accessed and utilized by language models. They extend the capabilities of language models by enabling them to perform actions beyond text generation.\n",
        "\n",
        "**Defining Tools:** Tools are defined using the `@tool` decorator from `langchain_core.tools`. This decorator allows you to specify the tool's name, description, and the function it executes.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Let's create a simple tool to get the current weather for a given city:"
      ],
      "metadata": {
        "id": "lymq31AwZMwy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.tools import tool\n",
        "import requests\n",
        "\n",
        "# A function to get weather of a city\n",
        "@tool(description=\"Get the current weather in a given location\")\n",
        "def get_weather(location: str) -> str:\n",
        "    return f\"It's 15 degrees in {location}.\"\n",
        "\n",
        "# A function to multiply two numbers\n",
        "@tool(description=\"Multiply 2 numbers\")\n",
        "def multiply(a: float, b: float) -> float:\n",
        "    \"\"\"Multiply two numbers.\"\"\"\n",
        "    return a - b"
      ],
      "metadata": {
        "id": "LH-Tg1cGcFQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [get_weather, multiply]"
      ],
      "metadata": {
        "id": "JwRhyMxDcGT9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(tools)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-2tULunVDrQ",
        "outputId": "72fa0727-48be-46f0-c512-e989bf516025"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2 Tool Binding\n",
        "\n",
        "**Concept:** Tool binding involves associating tools with a language model, allowing the model to call these tools when needed. This enables the language model to leverage external functionalities and data sources to complete tasks.\n",
        "\n",
        "**Binding Tools:** The `bind_tools` method of the `ChatOpenAI` class is used to bind tools to a language model. This creates a new instance of the language model with access to the specified tools.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Let's bind the `get_city_weather` tool to our language model:"
      ],
      "metadata": {
        "id": "M97Sw0UChSUl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\")\n",
        "llm_with_tools = llm.bind_tools(tools)"
      ],
      "metadata": {
        "id": "DKGQLxPbg1po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "1. **Binding:** The `bind_tools` method is called on the `model` object, passing a list containing the `get_city_weather` tool.\n",
        "2. **New Instance:** This creates a new language model instance named `llm_with_tools` that has access to the specified tool.\n",
        "\n",
        "Now, the language model can use the `get_city_weather` tool during its interactions, extending its capabilities beyond text generation."
      ],
      "metadata": {
        "id": "lShAtPRJhWql"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Tool Calling\n",
        "\n",
        "Tool calling involves invoking the bound tools within the context of a language model interaction. When the language model determines that a tool is necessary to complete a task, it calls the appropriate tool and uses the results in its response.\n",
        "\n",
        "**Calling Tools:** The process of tool calling is managed by LangChain's agent framework, which we will explore in the next subsection."
      ],
      "metadata": {
        "id": "6DvGO8A-hd-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ai_msg = llm_with_tools.invoke(\"What's the weather in London?\")"
      ],
      "metadata": {
        "id": "8_y5U5jkhXZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the tool calls in the response\n",
        "print(ai_msg.tool_calls)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JNf6AppVpu6Y",
        "outputId": "f34f955f-1fa3-4361-a607-491d2d87c1d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'name': 'get_weather', 'args': {'location': 'London'}, 'id': 'b3ea0e61-1cc5-4732-989b-57cefa3fe7bf', 'type': 'tool_call'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example tool call message would be needed here if you were actually running the tool\n",
        "from langchain_core.messages import ToolMessage, HumanMessage\n",
        "\n",
        "# Assuming the original user message is available as user_message\n",
        "# For this example, let's assume the previous human message was \"What's the weather in London?\"\n",
        "user_message = HumanMessage(content=\"What's the weather in London?\")\n",
        "\n",
        "\n",
        "tool_message = ToolMessage(\n",
        "    content=get_weather(*ai_msg.tool_calls[0][\"args\"]),\n",
        "    tool_call_id=ai_msg.tool_calls[0][\"id\"],\n",
        ")\n",
        "\n",
        "# Pass the original user message, the AI's response with the tool call,\n",
        "# and the tool message containing the result back to the model\n",
        "response_with_tool_result = llm_with_tools.invoke([user_message, ai_msg, tool_message])\n",
        "print(\"\\nResponse after tool execution:\")\n",
        "print(response_with_tool_result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMHSaQ7Won9H",
        "outputId": "4aba491c-cbc8-4000-8a39-0cbbf74278df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-16-2554436812.py:10: LangChainDeprecationWarning: The method `BaseTool.__call__` was deprecated in langchain-core 0.1.47 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  content=get_weather(*ai_msg.tool_calls[0][\"args\"]),\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Response after tool execution:\n",
            "The weather in London is sunny.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4 Agent\n",
        "\n",
        "Now that we have defined tools and bound them to our language model, we need a way to orchestrate their use. This is where agents come in.\n",
        "\n",
        "**Concept:** Agents in LangChain act as orchestrators, deciding which tools to use and when to use them to achieve a specific goal. They interact with the language model, receive its instructions, and execute actions using the available tools.\n",
        "\n",
        "**Creating Agents:** LangChain provides various types of agents, each with its own strategy for selecting and using tools. We will be using the **ReAct (Reasoning and Acting)** agent, which is known for its effectiveness in many scenarios.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Let's create a ReAct agent using LangGraph, a library for constructing and visualizing agents:\n",
        "\n",
        "Now that we have defined the tools and the LLM, we can create the agent. We will be using LangGraph to construct the agent."
      ],
      "metadata": {
        "id": "_AbOeFbtp7z6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import create_react_agent\n",
        "agent_executor = create_react_agent(llm, tools)\n",
        "response = agent_executor.invoke(\n",
        "    {\n",
        "        \"messages\": [\n",
        "            HumanMessage(content=\"hi! Whats the weather in kathmandu today, and tell me the product of 2 and 8\")\n",
        "            ]\n",
        "    }\n",
        "    )\n",
        "response[\"messages\"][-1].content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nn58n6YlqKuR",
        "outputId": "55744f46-6285-410a-ae4d-c6c0c091348e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The weather in Kathmandu is 15 degrees. The product of 2 and 8 is -6.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "w4DVCoKZqhzA",
        "outputId": "d392bde0-5f76-4f29-c341-ebbec29e01c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"It's sunny in London.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "\n",
        "1. **Import:** We import the `create_react_agent` function from `langgraph.prebuilt`.\n",
        "2. **Agent Creation:** We create a ReAct agent using `create_react_agent`, providing the language model (`model`) and the list of available tools (`[get_city_weather]`).\n",
        "3. **Invoking the Agent:** We invoke the agent using the `invoke` method, passing a dictionary containing a list of messages (in this case, a single `HumanMessage` with the user's question).\n",
        "4. **Accessing the Response:** The agent's response is stored in the `response` dictionary. We access the content of the last message (the agent's final answer) using `response[\"messages\"][-1].content`.\n",
        "\n",
        "**How ReAct Works:**\n",
        "\n",
        "The ReAct agent follows a loop of:\n",
        "1. **Thought:** The language model thinks about what action to take next.\n",
        "2. **Action:** It selects a tool to use based on its thoughts.\n",
        "3. **Observation:** It executes the tool and observes the results.\n",
        "4. **Thought:** It reflects on the observation and decides on the next action.\n",
        "\n",
        "This loop continues until the agent determines it has achieved the goal or can no longer make progress.\n",
        "\n",
        "**Further Learning:**\n",
        "\n",
        "- **LangChain Agents Documentation:** For a deeper dive into agents and their different types, refer to the official LangChain documentation.\n",
        "- **LangGraph Documentation:** To learn more about LangGraph and how to construct and visualize agents, consult the LangGraph documentation.\n",
        "\n",
        "By utilizing agents, you can build more complex and powerful AI applications that can access external resources and perform actions beyond the capabilities of language models alone. I hope this detailed markdown explanation of Section 2.4: Agent provides a clear and engaging learning experience for your students. Let me know if you have any adjustments or want to proceed with the next subsection. I'm ready to continue assisting you."
      ],
      "metadata": {
        "id": "VtVTqJrzqoJr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Question to solve**\n",
        "Create two tools: One for converting currency to current rate and the other that tells you if today is weekend or not."
      ],
      "metadata": {
        "id": "Ueinc1YNq2bv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# To Get Started\n",
        "import requests\n",
        "response = requests.get(\"https://api.frankfurter.app/latest?amount=1&from=USD&to=JPY\")\n",
        "response.json()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWsWc1Zfq3de",
        "outputId": "766f296e-7a0d-4dc4-80fa-3c166662e4ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'amount': 1.0, 'base': 'USD', 'date': '2025-07-29', 'rates': {'JPY': 148.77}}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qs-Bu1AprrDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qM82vGlsrrRp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Structure Output**\n",
        "\n",
        "In this section, we'll explore how to structure the output of language models using LangChain. This means going beyond simply getting a string response and instead getting a response that's in a specific format that's easier for your code to work with."
      ],
      "metadata": {
        "id": "mLbk5X1NrzBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pydantic import BaseModel, Field"
      ],
      "metadata": {
        "id": "HiuF2Mour1_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the desired structure\n",
        "class Person(BaseModel):\n",
        "    \"\"\"Information about a person.\"\"\"\n",
        "    name: str = Field(..., description=\"The person's name\")\n",
        "    height_m: float = Field(..., description=\"The person's height in meters\")"
      ],
      "metadata": {
        "id": "d0ss8mXYsEG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\", temperature=0)\n",
        "structured_llm = llm.with_structured_output(Person)\n",
        "\n",
        "# Invoke the model with a query asking for structured information\n",
        "result = structured_llm.invoke(\n",
        "    \"Who was the 16th president of the USA, and how tall was he in meters?\"\n",
        ")\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qoe_kQkNsIUa",
        "outputId": "d0e57c12-bb79-4d4b-eebf-65da254129b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "name='Abraham Lincoln' height_m=1.93\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question to solve\n",
        "Given a product description, extract the product name, price, and key features in a structured JSON format.\n",
        "\n",
        "## Product text:\n",
        "The AeroLite Pro Wireless Headphones offer a premium audio experience with active noise cancellation, 40-hour battery life, and ultra-soft memory foam ear cushions. Designed for both audiophiles and casual listeners, the AeroLite Pro also includes touch controls, a built-in microphone, and fast USB-C charging. Now available for $129.99, it combines style, comfort, and performance in one sleek package.\n",
        "\n",
        "## Output format:\n",
        "Name: str\n",
        "\n",
        "Price: float\n",
        "\n",
        "KeyFeatures: List(str)"
      ],
      "metadata": {
        "id": "9URjvGQwtZms"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "giMQskCAtXQs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}